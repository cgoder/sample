{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# print(os.environ)\n",
    "\n",
    "AI_TEMPERATURE=0.1\n",
    "AI_MAX_TOKENS=2048"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "def load_azureLLM():\n",
    "    # # chat\n",
    "    # chat = AzureChatOpenAI(\n",
    "    #     deployment_name=os.environ.get('DEPLOYMENT_NAME_CHAT'),\n",
    "    #     temperature=AI_TEMPERATURE,\n",
    "    #     max_tokens=AI_MAX_TOKENS,\n",
    "    #     )\n",
    "    # llm\n",
    "    llm = AzureOpenAI(\n",
    "        deployment_name=os.environ.get('DEPLOYMENT_NAME_LLM'),\n",
    "        model_name=\"text-davinci-003\",\n",
    "        temperature=AI_TEMPERATURE,\n",
    "        max_tokens=AI_MAX_TOKENS,\n",
    "        )\n",
    "    \n",
    "    embedding = OpenAIEmbeddings(deployment = os.environ.get('DEPLOYMENT_NAME_EMBEDDING'),chunk_size=1)\n",
    "    \n",
    "    return llm,embedding\n",
    "\n",
    "llm,embedding = load_azureLLM()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader, TextLoader, CSVLoader,\n",
    "    UnstructuredEPubLoader, UnstructuredWordDocumentLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def extract_pdf_content(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load_and_split()\n",
    "\n",
    "\n",
    "def extract_word_content(file_path):\n",
    "    loader = UnstructuredWordDocumentLoader(file_path, mode=\"elements\")\n",
    "    return loader.load_and_split()\n",
    "\n",
    "\n",
    "def extract_csv_content(file_path):\n",
    "    loader = CSVLoader(file_path)\n",
    "    return loader.load_and_split()\n",
    "\n",
    "\n",
    "def extract_epub_content(file_path):\n",
    "    loader = UnstructuredEPubLoader(file_path, mode=\"elements\")\n",
    "    return loader.load_and_split()\n",
    "\n",
    "\n",
    "def extract_md_content(file_path):\n",
    "    loader = UnstructuredMarkdownLoader(file_path, mode=\"elements\")\n",
    "    return loader.load_and_split()\n",
    "\n",
    "\n",
    "def extract_txt_content(file_path):\n",
    "    loader = TextLoader(file_path, encoding=\"utf8\")\n",
    "    return loader.load_and_split()\n",
    "\n",
    "def extract_file_content(file):\n",
    "    file_extension = os.path.splitext(file)[1]\n",
    "    documents = []\n",
    "    \n",
    "    with open(file, mode='rb') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "        temp_file.write(file_content)\n",
    "        temp_file_name = temp_file.name\n",
    "\n",
    "    loaders = {\n",
    "        \".pdf\": extract_pdf_content,\n",
    "        \".xls\": extract_csv_content,\n",
    "        \".xlsx\": extract_csv_content,\n",
    "        \".csv\": extract_csv_content,\n",
    "        \".docx\": extract_word_content,\n",
    "        \".epub\": extract_epub_content,\n",
    "        \".md\": extract_md_content,\n",
    "        \".txt\": extract_txt_content\n",
    "    }\n",
    "    if file_extension in loaders:\n",
    "        documents = loaders[file_extension](temp_file_name)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    temp_file.close()\n",
    "    os.remove(temp_file_name)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加文档\n",
    "> 遍历目录下所有文件，并分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data'\n",
    "\n",
    "docs = []\n",
    "docs_id =  []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            print(file)\n",
    "            docs_id.extend(filenames)\n",
    "            content = extract_file_content(os.path.join(dirpath, file))\n",
    "            docs.extend(content)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: \",e)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示分割好的文档\n",
    "# docs\n",
    "docs_id\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档向量化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# %pip install chromadb\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "DB_DIR = 'db'\n",
    "DB_VECTOR_NAME = \"langchain_db_chroma\"\n",
    "DB_VECTOR_NAME_FAISS = \"langchain_db_faiss\"\n",
    "\n",
    "ABS_PATH = os.path.abspath(os.getcwd())\n",
    "# ABS_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "DB_DIR = os.path.join(ABS_PATH, DB_DIR)\n",
    "\n",
    "def load_vectorDB():\n",
    "    if not os.path.exists(DB_DIR):\n",
    "        os.mkdir(DB_DIR)\n",
    "\n",
    "    # vectorstore = Chroma(persist_directory=DB_DIR,\n",
    "    #                      embedding_function=embedding)\n",
    "    \n",
    "    vectorstore = FAISS.load_local(DB_VECTOR_NAME_FAISS, embedding)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def save_vectorDB(docs, embedding):\n",
    "    if not os.path.exists(DB_DIR):\n",
    "        os.mkdir(DB_DIR)\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(\n",
    "    #     collection_name=DB_VECTOR_NAME,\n",
    "    #     documents=docs,\n",
    "    #     embedding=embedding,\n",
    "    #     ids=docs_id,\n",
    "    #     persist_directory=DB_DIR)\n",
    "    # vectorstore.persist()\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(docs,embedding)\n",
    "    vectorstore.save_local(DB_VECTOR_NAME_FAISS)\n",
    "    \n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = load_vectorDB()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存文档到向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = save_vectorDB(docs,embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对话链构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 4\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = [\"书名叫什么？\", \"出版社是哪家？\", \"作者是谁？\", \"作者致谢了多少人？\", \"作者致谢了谁？\"]\n",
    "# chat_history = []\n",
    "\n",
    "# for question in questions:  \n",
    "#     result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "#     chat_history.append((question, result['answer']))\n",
    "#     print(f\"-> **Question**: {question} \\n\")\n",
    "#     print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与文档对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_history = []\n",
    "\n",
    "def ask_question(user_input):\n",
    "    result = qa({\"question\": user_input, \"chat_history\": chat_history})\n",
    "    chat_history.append((user_input, result['answer']))\n",
    "    print(f\"-> **Question**: {user_input} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"欢迎使用问答系统，请输入您要提问的问题，按回车键确认。\")\n",
    "while True:\n",
    "    user_input = input(\"> \").strip()  # 获取用户输入的问题\n",
    "    if user_input == \"exit\":\n",
    "        break  # 用户输入 exit，退出程序\n",
    "    elif user_input.strip():\n",
    "        ask_question(user_input)\n",
    "    else:\n",
    "        print(\"对不起，我暂时无法回答这个问题，请尝试其它问题。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
